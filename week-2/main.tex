\documentclass[hyperref={pdfpagelabels=false}]{beamer}
%
% use packages
\usepackage{lmodern}
\usepackage{hyperref} 
%
% use theme
\usetheme{Boadilla}
%
% global commands
\newcommand{\C}{\mathbb{C}} 
\newcommand{\F}{\mathbb{F}} 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\bbone}{\ensuremath{\mathbbm{1}}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\U}{\mathsf}
\newcommand{\celsius}{\ensuremath{^{\circ}\mathsf{C}}}
\newcommand{\myseries}[2]{#1_1,\ldots,#1_{#2}}
\newcommand{\highlightr}[1]{\textcolor[rgb]{1,0.3,0.2}{\emph{\textbf{#1}}}}
\newcommand{\highlightg}[1]{\textcolor[rgb]{0.1,0.5,0.3}{\emph{\textbf{#1}}}}
\newcommand{\structb}[1]{\textcolor[rgb]{0.2,0.2,0.7}{#1}}
\newcommand{\red}[1]{\textcolor[rgb]{1,0.3,0.2}{#1}}
\newcommand{\bemph}[1]{\emph{\textbf{#1}}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\renewcommand\arraystretch{1.8}
%
% title page parameter
\title{VE401, Probabilistic Methods in Eng.}
\subtitle{Recitation Class - Week 2}
\author[Z Zhou]{Zhanpeng Zhou}
\institute[UMJI-SJTU]{UMJI-SJTU Joint Institute}
\date{\today}
\logo{\includegraphics[scale=0.14]{ji_logo.png}}
\def\vfilll{\vskip 0pt plus 1filll minus 0pt }
%
% begin page of each subsection
\AtBeginSubsection[]{
    \begin{frame}
    \tableofcontents[
            sections={1-6},
            currentsection,
            hideothersubsections]
    \end{frame}
}
%% begin document
\begin{document}
%
% title page
\begin{frame}
    \vfilll
    \titlepage
    \vfilll
    \usebeamerfont{institute} Mail: \href{mailto:zzp1012@sjtu.edu.cn}{zzp1012@sjtu.edu.cn} / WeChat: zzp01012
\end{frame}
%
% table of content
\begin{frame}{Table of contents}
    \tableofcontents[hideothersubsections]
\end{frame}
%
% Section: elementary probability
\section{Elementary Probability}
%
% Subsection: sample space and probability
\subsection{Sample Space and Probability}
%
% Frame: the definition of sample space and events
\begin{frame}{Sample Space and Events}
    \structb{Definitions.}
    \begin{itemize}
    	\justifying
    	\item \highlightg{Sample space}: a space containing all possible outcomes of an experiment.
    	\item \highlightg{Event}: a subset of sample space, containing possible outcomes of the experiment.
    	\item \highlightg{$\sigma$-field}: $\mathcal{F}$ on $S$ is a family of subsets of $S$ s.t.
    	\begin{enumerate}
    		\justifying
    		\item $\emptyset\in \mathcal{F}$;
    		\item if $A\in \mathcal{F}, \mathsf{then\ } S\setminus A \in \mathcal{F}$;
    		\item if $A_1, A_2, \ldots \in \mathcal{F}$ is a finite or countable sequence of subsets, then the union $\bigcup_k A_k\in \mathcal{F}$.
    	\end{enumerate}
    \end{itemize}
\end{frame}
%
% Frame: Example of sample space and events
\begin{frame}{Sample Space and Events}
    \structb{Example 1.} Suppose a coin is tossed three times. Then the sample space $S$ contains the following possible outcomes:
    \begin{align*}
        s_1 = HHH, \quad s_2 = THH, \quad s_3 = HTH, s_4 = HHT, \\
        s_5 = HTT, \quad s_6 = THT, \quad s_7 = TTH, s_8 = TTT,
    \end{align*}
    where $H$ denotes head and $T$ denotes tail. Then the event that at most two tails are obtained is given by
    \pause
    \begin{align*}
        A = \{s_1, s_2, s_3, s_4, s_5, s_6, s_7\}.
    \end{align*}
\end{frame}
%
% Frame: probability measure and spaces
\begin{frame}{Probability Measures and Spaces}
    \justifying
    \structb{Definition.} Let $S$ be a sample space and $\mathcal{F}$ a $\sigma$-filed on $S$. Then function
    \begin{align*}
        P: \mathcal{F} \rightarrow [0, 1], \qquad A\mapsto P[A]
    \end{align*}
    is a \highlightg{probability measure} on $S$ if
    \begin{enumerate}
    	\justifying
    	\item[(i)] $P[S] = 1$,
    	\item[(ii)] For any set of events $\{A_k\} \in \mathcal{F}$ such that $A_j\cap A_k = \emptyset$ for $j\neq k$,
    	\begin{align*}
    	    P\left[\underset{k}{\bigcup}A_k \right] = \sum_k P[A_k].
    	\end{align*}
    \end{enumerate}
    The triple $(S, \mathcal{F}, P)$ is called a \highlightg{probability space}.
\end{frame}
%
% Frame: the basic property of probability measure.
\begin{frame}{Probability Measures and Spaces}
    \justifying
    \structb{Properties.} 
    \begin{itemize}
    	\justifying
    	\item $P[S] = 1$,
    	\item $P[\emptyset] = 0$,
    	\item $P[S\setminus A] = 1 - P[A]$,
    	\item $P[A_1\cup A_2] = P[A_1] + P[A_2] - P[A_1\cap A_2]$,
    \end{itemize}
    where $A, A_1, A_2, \in S$ are any events.
\end{frame}
%
% Subsection: Cardano's Principle 
% Frame: Cardano's Principle
\subsection{Cardano's Principle}
\begin{frame}{Cardano's Principle}
    \justifying
    \structb{Intuition}: The probability of an outcome $A$ is the proportion of the ways that can lead to $A$, given that all the ways are equally likely and mutually exclusive.
    ~\\
    \highlightg{Cardano's Principle}:
    \begin{align*}
        P[A] = \dfrac{\U{Number\ of\ ways\ leading\ to\ outcome} A}{\U{Number\ of\ ways\ experiments\ can\ proceed}}
    \end{align*}
    ~\\
    \structb{Example 2.} Tossing two (unbiased) coins, $P[\U{getting\ one\ head}]$ \pause = $\dfrac{2}{4}$ = 0.5.
\end{frame}
%
% Subsection: Counting Methods
% Frame: Basic Principles of Counting
\subsection{Counting Methods}
\begin{frame}{Basic Principles of Counting}
    Suppose a set $A$ of $n$ objects is given.
    \begin{itemize}
    	\justifying
    	\item \highlightg{Permutation of $k$ objects}: $\dfrac{n!}{(n-k)!}$ ways of choosing an \underline{ordered} tuple of $k$ objects from $A$.
    	\item \highlightg{Combination of $k$ objects}: $\dfrac{n!}{k!(n-k)!}$ ways of choosing an \underline{unordered} set of $k$ objects from $A$.
    	\item \highlightg{Permutation of $k$ indistinguishable objects}: $\dfrac{n!}{n_1!n_2!\ldots n_k!}$ ways of partitioning $A$ into $k$ disjoint subsets $A_1, \ldots, A_k$ whose union is $A$, where each $A_i$ has $n_i$ elements.
    \end{itemize}
\end{frame}
%
% Frame: Example of Counting
\begin{frame}{Basic Principles of Counting}
    \justifying
    \structb{Example 3.} If the letters \emph{s, s, s, t, t, t, i, i, a, c} are arranged in a random order, what is the probability that they will spell the word ``statistics''?
    \uncover<2>{
    ~\\
    ~\\
    \structb{Solution.} The problem is equivalent to arranging 5 disjoint subsets, giving the probability
    \begin{align*}
        p = 1\Bigg/\frac{10!}{3!\times 3!\times 2!\times 1\times 1} = \frac{1}{50400}.
    \end{align*}
    }
\end{frame}
%
% Section: Conditional Probability
\section{Conditional Probability}
%
% subsection definition of conditional probability
\subsection{Conditional Probability}
%
% Frame: conditional Probability
\begin{frame}{Conditional Probability}
    \structb{Definitions}
    \begin{itemize}
    	\justifying
    	\item \highlightg{Conditional probability} of ``$B$ occurs given $A$ has occurred'': 
    	\begin{align*}
    	    P[B|A] = \frac{P[B\cap A]}{P[A]}
    	\end{align*}
    	\item \highlightg{Total probability} for $P[B]$ on a sample space $S$, given events $A_1, \ldots, A_n \in S$ are mutually exclusive and $A_1\cup\cdots\cup A_n = S$:
    	\begin{align*}
    	    P[B] = \sum_{k=1}^{n} P[B|A_k]\cdot P[A_k].
    	\end{align*}
    	\item \highlightg{Independence} of events $A$ and $B$: $P[A\cap B] = P[A]P[B]$, which is equivalent to
    	\begin{align*}
        	P[A|B] = P[A] \qquad \U{if}\ P[B]\neq 0, \\
        	P[B|A] = P[B] \qquad \U{if}\ P[A]\neq 0.
    	\end{align*}
    \end{itemize}
\end{frame}
%
% Frame: conditional probability
\begin{frame}{Conditional Probability}
    \justifying
    \structb{Example 4.} Suppose Keven plays a game where his score must be 1, 2, \ldots, 50, and each of these 50 scores is equally likely. Suppose he get $X$ for his first try. He then continue to play the game until he obtains another score $Y$ such that $Y\geq X$. What is the probability of the event $A$ that $Y = 50$?
    ~\\
    ~\\
    \pause
    \justifying
    \structb{Solution.} For each $i = 1, \ldots, 50$, let $B_i$ be the event that $X = i$. Then conditional on $B_i$, the value of $Y$ is equally likely to be any one of the numbers $i, i + 1, \ldots, 50$. Therefore, the probability is given by
    \begin{align*}
        P[A] = \sum_{i=1}^{50} P[B_i]\cdot P[A|B_i] = \sum_{i=1}^{50} \frac{1}{50}\cdot \frac{1}{51-i} \approx 0.09.
    \end{align*}
\end{frame}
%
% Frame: Independence
\begin{frame}{Independence}
    \justifying
    \structb{Example 5.} Given events $A$ and $B$, what happens to $P[A \cap B]$, $P[A \cup B]$, $P[A | B]$, $P[A \ \neg B]$, and $P[\neg A | B]$? You are encouraged to fill out this table by yourself first.
    ~\\
    \centering
    \begin{tabular}{c|c|c}
        $A$ and $B$ are ... & mutually exclusive & independent \\
        $P[A \cap B]$ & & \\
        $P[A \cup B]$ & & \\
        $P[A | B]$ & & \\
        $P[A | \neg B]$ & & \\
        $P[\neg A | B]$ & & 
    \end{tabular}
\end{frame}
%
% Frame: Independence
\begin{frame}{Independence}
    \justifying
    \structb{Example 5.} Given events $A$ and $B$, what happens to $P[A \cap B]$, $P[A \cup B]$, $P[A | B]$, $P[A \ \neg B]$, and $P[\neg A | B]$? You are encouraged to fill out this table by yourself first.
    ~\\
    \centering
    \begin{tabular}{c|c|c}
        $A$ and $B$ are ... & mutually exclusive & independent \\
        $P[A \cap B]$ & $0$ & $P[A]P[B]$ \\
        $P[A \cup B]$ & $P[A] + P[B]$ & $P[A] + P[B] - P[A]P[B]$ \\
        $P[A | B]$ & $0$ & $P[A]$ \\
        $P[A | \neg B]$ & $\frac{P[A]}{1 - P[B]}$ & $P[A]$ \\
        $P[\neg A | B]$ & $1$ & $1 - P[A]$ 
    \end{tabular}
\end{frame}
%
% Frame: Total Probability
\begin{frame}{Total Probability}
    \justifying
    \structb{Example 6.} The Marriage Problem: Optimal strategy: For some $r \geq 1,$ evaluate and automatically reject $r-1$ potential partners.Then select the first candidate superior to all the previous ones, if possible.
    \begin{enumerate}
        \item Choose $r \geq 1$
        \item Select $k$ with $y_{k}=1$ and $k \geq r,$ if possible. Discard all others.
        \item Otherwise, do not choose anyone.
    \end{enumerate}
    The sample space can be taken to be $S=\left\{(k, j): k \text { is selected and } x_{j}=1, k=0, \dots, n, j=1, \dots, n\right\}$. We say that we "win" if we end up by selecting the most suitable partner, i.e., we select $k$ with $x_{k}=1 .$ We denote this event by
    $$
        W_{r}=\{(k, k): k=r, \dots, n\}, \quad r \geq 1
    $$
\end{frame}
%
% Frame: Total Probability
\begin{frame}{Total Probability}
    \justifying
    Given $r \geq 1,$ the probability of winning is denoted
    $$
        p_{r}=P\left[W_{r}\right]
    $$
    \structb{Goal: }Choose $r$ so that $p_{r}$ is maximal.
    ~\\
    $$
        p_{r}=P\left[W_{r}\right]=\sum_{m=1}^{n} P\left[W_{r} \mid B_{m}\right] P\left[B_{m}\right]
    $$
    $$
        B_m = \{(k, m): k = 0, \cdots, n\}, m = 1, \cdots, n.
    $$
    \highlightr{Note: }: $P\left[B_{m}\right]$: the probability that the $m$th person is most suitable partner "in reality".
    ~\\
    $P\left[W_{r} | B_{m}\right]$: the probability of choosing the most suitable partner "in reality", given that the $m$th person is most suitable partner "in reality".
\end{frame}

%
% Subsection: Bayes's theorem
\subsection{Bayes's Theorem}
%
% Frame: bayes's theorem
\begin{frame}{Bayes's Theorem}
    \justifying
    \structb{Theorem.} Let $A_1, \ldots, A_n \subset S$ be a set of pairwise mutually exclusive events whose union is $S$ and who each have non-zero probability of occurring. Let $B\subset S$ be any event such that $P[B]\neq 0$. Then for any $A_k, k = 1, \ldots, n$,
    \begin{align*}
        P[A_k|\red{B}] = \frac{P[B\cap A_k]}{P[B]} = \frac{P[B|\red{A_k}]\cdot P[A_k]}{\sum_{j=1}^n P[B|\red{A_j}] \cdot P[A_j]}.
    \end{align*}
\end{frame}
%
% Frame: bayes's theorem
\begin{frame}{Bayes's Theorem}
    \justifying
    \structb{Example 7.} A box contains one fair coin and one coin with heads on both sides. Suppose one coin is selected at random and when it is tossed twice, two heads are obtained. What is the probability that the coin is the fair coin?
    ~\\
    ~\\
    \pause
    \justifying
    \structb{Solution.} Let $E_1$ be the event that the selected coin is fair, and $E_2$ be the event that the selected coin have two heads. Using Bayes's theorem, we have
    \begin{align*}
        P[E_1|HH] & = \frac{P[E_1]P[HH|E_1]}{P[HH|E_2]P[E_2] + P[HH|E_1]P[E_1]} \\
        & = \frac{\dfrac{1}{2}\cdot \dfrac{1}{4}}{1\cdot \dfrac{1}{2} + \dfrac{1}{4}\cdot\dfrac{1}{2}} = \frac{1}{5}.
    \end{align*}
\end{frame}
%
% Section Discrete Random Variables
\section{Discrete Random Variables}
%
% Subection Random Variables and Probability Density Function
\subsection{Random Variables and Probability Density Function}
\begin{frame}{Random Variables and Probability Density Function}
    \justifying
    \structb{Definition.} Let $S$ be a sample space and $\Omega$ a \underline{countable} subset of $\R$. A \highlightg{discrete random variable} is a map
    \begin{align*}
        X: S\rightarrow \Omega
    \end{align*}
    together with a function
    \begin{align*}
        f_X: \Omega \rightarrow \R
    \end{align*}
    having the properties that
    \begin{itemize}
    	\item[(i)] $f_X(x) \geq 0$ for all $x\in \Omega$ and
    	\item[(ii)] $\displaystyle \sum_{x\in \Omega} f_X(x) = 1$.
    \end{itemize}
    The function $f_X$ is called the \highlightg{probability density function} or \highlightg{probability distribution} of $X$. A random variable is given by the pair $(X, f_X)$.
\end{frame}
%
% Subsection: Cumulative Distribution Function
\subsection{Cumulative Distribution Function}
\begin{frame}{Cumulative Distribution Function}
    \justifying
    \structb{Definition.} The \highlightg{cumulative distribution function} of a random variable is defined as
    \begin{align*}
        F_X: \R\rightarrow \R, \qquad F_X(x) := P[X\leq x].
    \end{align*}
    For a discrete random variable,
    \begin{align*}
        F_X(x) = \sum_{y\leq x} f_X(y).
    \end{align*}
\end{frame}
%
% Subsection: Expectation and Variance
\subsection{Expectation and Variance}
%
% Frame: Expectation and Variance
\begin{frame}{Expectation and Variance}
    \structb{Definition.} Let $(X, f_X)$ be a discrete random variable.
    \begin{itemize}
    	\justifying
    	\item The \highlightg{expected value} or \highlightg{expectation} of $X$ is 
    	\begin{align*}
    	    \mu_X = \U{E}[X] := \sum_{x\in \Omega} x\cdot f_X(x),
    	\end{align*}
    	provided that the sum (possibly series, if $\Omega$ is infinite) on the right converges absolutely.
    	\item The \highlightg{variance} is defined by
    	\begin{align*}
    	    \sigma_X^2 = \U{Var}[X] := \U{E}\left[(X - \U{E}[X])^2 \right]
    	\end{align*}
    	which is defined as long as the right-hand side exists.
    	\item The \highlightg{standard deviation} is $\sigma_X = \sqrt{\U{Var}[X]}$.
    \end{itemize}
\end{frame}
%
% Frame: Expectation and Variance's Properties
\begin{frame}{Properties}
    \begin{itemize}
    	\justifying
    	\item \underline{Expectation}.
    	\justifying
    	\begin{enumerate}[(a).]
    		\justifying
    		\item Suppose $\varphi: \Omega \rightarrow \R$ is some function, then
    		\begin{align*}
    		    \U{E}[\varphi\circ X] = \displaystyle \sum_{x\in \Omega} \varphi(x) \cdot f_X(x).
    		\end{align*}
    		\item $\U{E}[aX + bY + c] = a\U{E}[X] + b\U{E}[Y] + c$, where $a, b, c\in \R$ and $X, Y$ are random variables.
    		\item $\displaystyle\U{E}\left[\sum_{i=1}^n X_i \right]  = \sum_{i=1}^n \U{E}[X_i]$, if each expectation exists.
    		\item If $X_1, \ldots, X_n$ are independent random variables with finite expectations, and $g_i, i = 1, \ldots, n$ are functions, then 
    		\begin{align*}
    		    \U{E}\left[\prod_{i=1}^n X_i \right] = \prod_{i=1}^n \U{E}[X_i], \quad \U{E}\left[\prod_{i=1}^n g_i(X_i) \right] = \prod_{i=1}^n \U{E}[g_i(X_i)].
    		\end{align*}
    	\end{enumerate}
    \end{itemize}
\end{frame}
%
% Frame: Expectation and Variance's Properties
\begin{frame}{Properties}
    \begin{itemize}
    	\justifying
    	\item \underline{Variance}.
    	\begin{enumerate}[(a).]
    		\justifying
    		\item $\U{Var}[X] = \U{E}[X^2] - \U{E}[X]^2$.
    		\item $\U{Var}[aX + b] = a^2\U{Var}[X]$, where $a, b \in \R$.
    		\item If $X_1, \ldots, X_n$ are independent random variables, then 
    		\begin{align*}
    		    \U{Var}\left[\sum_{i=1}^n a_iX_i \right]  = \sum_{i=1}^n a_i^2\U{Var}[X_i].
    		\end{align*}
    	\end{enumerate}
    	\highlightr{Note.} If $X$ and $Y$ are not independent, then according to definitions,
    	\begin{align*}
        	\U{Var}[X + Y] & = \U{E}\left[\left(X + Y - (\mu_X + \mu_Y) \right)^2 \right] \\
        	& = \U{E}\left[(X-\mu_X)^2 \right] + \U{E}\left[(Y - \mu_Y)^2 \right] + \\
        	& \qquad \qquad \qquad + 2\U{E}\left[(X-\mu_X)(Y-\mu_Y) \right] \\
        	& \neq \U{Var}[X] + \U{Var}[Y].
    	\end{align*}
    \end{itemize}
\end{frame}
%
% Subsection: Moment-Generating Function
\subsection{Moment-Generating Function}
%
% Frame: Ordinary and Central Moments
\begin{frame}{Ordinary and Central Moments}
    \justifying
    \structb{Definition.} The \highlightg{$n^{th}$ (ordinary) moments} of a random variable $X$ is given by
    \begin{align*}
        \U{E}[X^n], \qquad n\in \N.
    \end{align*}
    The \highlightg{$n^{th}$ central moments} of $X$ is given by
    \begin{align*}
        \U{E}\left[\left(\frac{X-\mu}{\sigma} \right)^n \right], \qquad \U{where\ } n = 3, 4, 5, \ldots
    \end{align*}
\end{frame}
%
% Frame: Moment-Generating Function
\begin{frame}{Moment-Generating Function}
    \justifying
    \structb{Definition.} Let $(X, f_X)$ be a random variable and such that the sequence of moments $\U{E}[X^n], n\in \N$, exists. If the power series 
    \begin{align*}
        m_X(t) := \sum_{k=0}^{\infty} \frac{\U{E}[X^k]}{k!} t^k
    \end{align*}
    has radius of convergence $\varepsilon > 0$, the thereby defined function
    \begin{align*}
        m_X(t): (-\varepsilon, \varepsilon) \rightarrow \R
    \end{align*}
    is called the \highlightg{moment-generating function} for $X$.
\end{frame}


\begin{frame}{Moment-Generating Function}

\justifying
\structb{Theorem.} Let $\varepsilon > 0$ be given such that $\U{E}[e^{tX}]$ exists and has a power series expansion in $t$ that converges for $|t| < \varepsilon$. Then the moment-generating function exists and 
\begin{align*}
m_X(t) = \U{E}[e^{tX}] \qquad \U{for\ } |t| < \varepsilon.
\end{align*}
Furthermore, 
\begin{align*}
E[X^k] = \frac{\U{d}^k m_X(t)}{\U{d}t^k}\bigg|_{t=0}.
\end{align*}
We can hence calculate the moments of $X$ by differentiating the moment-generating function.

\end{frame}

\begin{frame}{Moment-Generating Function}

\structb{Properties.} 
\begin{itemize}
	\justifying
	\item $X$ is a random variable and $Y = aX + b, a, b\in \R$, then for every $t$ such that $m_X(at)$ is finite,
	\begin{align*}
	m_Y(t) = e^{bt} m_X(at).
	\end{align*}
	\item Suppose $X_1, \ldots, X_n$ are $n$ independent random variables, then for every value that $m_{X_i}(t)$ is finite for all $i = 1, \ldots, n$,
	\begin{align*}
	m_X(t) = \prod_{i=1}^n m_{X_i}(t), \qquad X = X_1 + \cdots + X_n.
	\end{align*}
\end{itemize}

\end{frame}


\begin{frame}{Moment-Generating Function}

\justifying
\structb{Example 8.} Suppose that $X$ is a random variable with the moment-generating function 
\begin{align*}
m_X: \R\rightarrow \R, \qquad m_X(t) = e^{t^2 + 3t}.
\end{align*}
Find the mean and variance of $X$.\\
~\\
\uncover<2>{
	\structb{Solution.} We calculate
	\begin{align*}
	m_X'(t) = (2t+3) e^{t^2 + 3t}, \quad m_X''(t) = (2t+3)^2e^{t^2+3t} + 2e^{t^2+3t}.
	\end{align*}
	Therefore,
	\begin{align*}
	\mu = m_X'(0) = 3, \quad \sigma^2 = \U{E}[X^2] - \U{E}[X]^2 = m_X''(0) - \mu^2 = 2.
	\end{align*}
}

\end{frame}


\section{Common Distributions of Discrete Random Variables}

\subsection{Binomial Distribution}

\begin{frame}{Bernoulli Distribution}

\justifying
\structb{Definition.} A random variable $(X, f_X)$ has a \highlightg{Bernoulli distribution} with parameter $p, 0 < p < 1$ if the probability density function is defined by
\begin{align*}
f_X: \{0, 1\} \rightarrow \R, \qquad f_X(x) = \left\{
\begin{array}{ll}
1-p, & \U{if\ } x = 0, \\
p, & \U{if\ } x = 1.
\end{array}
\right.
\end{align*}
\structb{Interpretation.} Describe the probability of success $f_X(1)$ or failure $f_X(0)$ of a trial, given the probability of success is $p$.

\end{frame}

\begin{frame}{Bernoulli Distribution}

\justifying
\structb{Mean, variance, and M.G.F.}
\begin{itemize}
	\justifying
	\item \underline{Mean}.
	\begin{align*}
	\U{E}[X] = 0\cdot (1-p) + 1\cdot p = p.
	\end{align*}
	\item \underline{Variance}.
	\begin{align*}
	\U{Var}[X] = \U{E}[X^2] - \U{E}[X]^2 = p - p^2 = p(1-p).
	\end{align*}
	\item \underline{M.G.F.}
	\begin{align*}
	m_X: \R\rightarrow \R, \qquad m_X(t) = (1-p) + e^tp.
	\end{align*}
\end{itemize}

\end{frame}

\begin{frame}{Binomial Distribution}

\justifying
\structb{Definition.} A random variable $(X, f_X)$ has a \highlightg{binomial distribution} with parameter $n\in \N\setminus\{0\}$ and $p, 0 < p < 1$ if it has probability density function
\begin{align*}
f_X: \{0, \ldots, n\} \rightarrow \R, \qquad f_X(x) = \binom{n}{x} p^x(1-p)^{n-x}.
\end{align*}
~\\
\structb{Interpretation.} $f_X(x)$ is the probability of obtaining $x$ successes in $n$ independent and identical Bernoulli trials with parameter $p$.


\end{frame}


\begin{frame}{Binomial Distribution}

\justifying
\structb{Mean, variance and M.G.F.} 
\begin{itemize}
	\justifying
	\item \underline{Mean}.
	\begin{align*}
	\U{E}[X] & = \sum_{i=1}^n \U{E}[X_i] = np.
	\end{align*}
	\item \underline{Variance}. 
	\begin{align*}
	\U{Var}[X] = \sum_{i=1}^n\U{Var}[X_i] = np(1-p).
	\end{align*}
	\item \underline{M.G.F.}
	\begin{align*}
	m_X: \R\rightarrow\R, \qquad m_X(t) = \U{E}[e^{tX}] = \prod_{i=1}^n\U{E}[e^{tX_i}] = (1 - p + pe^t)^n.
	\end{align*}
\end{itemize}

\end{frame}

\subsection{Geometric Distribution}

\begin{frame}{Geometric Distribution}

\justifying
\structb{Definition.} A random variable $(X, f_X)$ has \highlightg{geometric distribution} with parameter $p, 0 < p < 1$ if the probability density function is given by
\begin{align*}
f_X: \N\setminus \{0\} \rightarrow \R, \qquad f_X(x) = (1-p)^{x-1} p.
\end{align*}
~\\
\structb{Interpretation.} $f_X(x)$ is the probability of $x$ failures before the first success in the Bernoulli trials, given the probability of success for each trial is $p$.

\end{frame}

\begin{frame}{Geometric Distribution}

\justifying
\structb{Mean, variance and M.G.F.} 
\begin{itemize}
	\justifying
	\item \underline{Mean}.
	\begin{align*}
	\U{E}[X] = \frac{1}{p}.
	\end{align*}
	\item \underline{Variance}.
	\begin{align*}
	\U{Var}[X] = \frac{1-p}{p^2}.
	\end{align*}
	\item \underline{M.G.F.}
	\begin{align*}
	m_X: (-\infty, -\ln(1-p)) \rightarrow\R, \qquad m_X(t) = \frac{pe^t}{1-(1-p)e^t}.
	\end{align*}
\end{itemize}

\end{frame}
%
% Section: Extended discussion problems
\section{Supplementary Discussions}
%
% Subsection: Banach Matchbox problem
\subsection{Banach Matchbox Problem}
\begin{frame}{Banach Matchbox Problem}
    \justifying
    \structb{Problem setup.} Suppose Keven carries two matchboxes at all times: one in his left pocket and one in his right. Each time he needs a match, he is equally likely to take it from either left or right pocket. Then one time he reaches into his pocket and discovers for the first time that the box picked is empty. Assuming each matchbox originally contained $n$ matches, what is the probability that there are exactly $k$ matches in the other box?
\end{frame}
%
% Frame: Banach Match
\begin{frame}{Banach Matchbox Problem}
    \justifying
    \structb{Probabilistic reasoning.} Keven has to have picked $2n-k$ times, plus 1 time for him to discover that one pocket is empty. Suppose for now the empty pocket is the right one, he has to pick right for $n$ times. The probability of this case is
    \begin{align*}
        P[L = k|R = 0] = \binom{2n-k}{n}\left(\frac{1}{2} \right)^{2n-k} \times \frac{1}{2}.
    \end{align*}
    Since the empty pocket can be either right or left, the total probability is then given by
    \begin{align*}
        P[R = k|L = 0] + P[L = k|R = 0] = \binom{2n-k}{n}\left(\frac{1}{2} \right)^{2n-k}.
    \end{align*}
\end{frame}
%
% Frame: Banach Match
\begin{frame}{Banach Matchbox Problem}
    \justifying
    \structb{Counting reasoning.} Suppose for now that the empty pocket is the right one, then Keven has to take $2n-k+1$ matches to notice an empty pocket. The number of ways that he picks those matches is $\dbinom{2n-k}{n}$. Using the counting method, the denominator should sum up all possible $k$s. Considering that the empty pocket can be either right or left, the probability of interest is given by
    \begin{align*}
        \frac{2\binom{2n-k}{n}}{2\sum_{i=0}^n\binom{2n-i}{n}} = \frac{\binom{2n-k}{n}}{\sum_{i=0}^n \binom{2n-i}{n}}.
    \end{align*}
    What has gone wrong?
    \pause
    ~\\
    \highlightr{Note.} It is important that the outcomes in the counting method are equally likely.
\end{frame}
%
% Frame: Banach Match
\begin{frame}{Banach Matchbox Problem}
    \justifying
    \structb{Counting reasoning.} The sample space indicated by this reasoning is 
    \begin{align*}
        S = \{k: \mathsf{there\ are\ } k \mathsf{\ matches\ left\ in\ the\ other\ pocket} \},
    \end{align*}
    which is not a \highlightg{simple sample space}. Namely, the outcomes are not \underline{equally likely}. Intuitively speaking, there are more likely that the other pocket is also empty ($k=0$) than the other pocket is totally untouched ($k=n$), given that he equally likely takes a match from left/right pocket.
\end{frame}
%
% Subsection: Two Envelops Problem
\subsection{Two Envelopes Problem}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Problem setup.} Cindy and Keven are given two indistinguishable envelopes, each of which contains a positive amount of money. One envelope contains twice as much as the other. They each may select and open one envelope and keep whatever amount it contains, but upon selection, are offered the chance to take the other envelope instead. If you are Cindy, what should you do?
\end{frame}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Ex ante argument.} Once the envelopes are ready to choose, the total amount of money has already been fixed to, say $Z$. The probability that you select the one with smaller/larger amount is 1/2. Therefore, the expectation should always be
    \begin{align*}
        E[X] = \frac{1}{2} \cdot \frac{1}{3}Z + \frac{1}{2} \cdot \frac{2}{3}Z = \frac{1}{2}Z,
    \end{align*}
    which is the case for both envelopes. Any problem with this argument?
    \uncover<2>{
    	~\\
    	\justifying
    	This is argument assumes that the action of opening the envelope is \emph{uninformative}. However, the problem arises after they have selected the envelopes.
    }
\end{frame}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Switching argument.} Let $x$ be the amount in the selected envelope. Then the probability that $x$ is the smaller/larger amount is 0.5, with the other envelope containing $2x$/$\dfrac{1}{2}x$, respectively. Thus the expectation of the money in the other envelope is
    \begin{align*}
        \frac{1}{2}\cdot 2x + \frac{1}{2} \cdot \frac{x}{2} = \frac{5}{4}x > x.
    \end{align*}
    Since this argument is true for both Cindy and Keven, they should both swap the envelopes. How come?
\end{frame}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Bayesian point of view.} As we mentioned earlier, the paradox arises when the probability that the other player has the higher/lower amount is 1/2 regardless whether we open the envelope. Suppose Cindy gets $x$ in her envelope, to remain the 1/2 probability, she has to assume that the combinations $\{x, 2x\}$ and $\{x/2, x\}$ are equally likely. This can be formalized as follows.
\end{frame}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Bayesian point of view (continued).} Suppose the prior probability for the smaller amount $M$ is $f$, and the envelopes contain $\{M, 2M\}$. Then we need to find the conditional probability
    \begin{align*}
        & P[\U{smaller\ amount\ } M = x|\U{Cindy\ gets\ } C = x] \\
        = & \frac{P[C = x|M = x] \cdot f(x)}{P[C = x|M = x]\cdot f(x) + P[C = x|M = x/2]\cdot f(x/2)} \\
        = & \frac{f(x)}{f(x) + f(x/2)}.
    \end{align*}
    For this to be $1/2$, we require that $f(x) = f(x/2)$ for any $x\in (0, \infty)$, which is nonsense, since that requires $f(x) = 0, \forall x\in (0, \infty)$.
\end{frame}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Bayesian point of view (continued).} Then if you are Cindy, what should be the strategy? The prior probability for the two events $\{x/2, x\}$ (event $A_1$) and $\{x, 2x\}$ (event $A_2$) are $f(x/2)$ and $f(x)$. Then the expectation of the amount in the other envelope should be
    \begin{align*}
        E & = \frac{x}{2}\cdot P[A_1|C = x] + 2x\cdot P[A_2|C = x] \\
        & = \frac{x}{2} \cdot \frac{f(x/2)}{f(x/2) + f(x)} + 2x \cdot \frac{f(x)}{f(x/2) + f(x)} \\
        & = \frac{x}{2}\cdot \frac{f(x/2) + 4f(x)}{f(x/2) + f(x)}.
    \end{align*}
    Let $E > x$, then Cindy should swap the envelope if $f(x) > \dfrac{1}{2}f(x/2)$.
\end{frame}
%
% Frame: Two Envelopes Problem
\begin{frame}{Two Envelopes Problem}
    \justifying
    \structb{Bayesian point of view (continued).} From yet another point of view (if you know about total expectation), to prove that the probability of the other envelope containing larger/smaller amount is no longer 0.5, we first assume that this is the case. Denoting the amounts as $X$ and $Y$.
    \begin{align*}
        E_{Y}[Y] = E_X\left[E_{Y|X}[Y|X]\right] = E_X\left[\frac{1}{2}\cdot \frac{1}{2}x + \frac{1}{2}\cdot 2x \right] = \frac{5}{4}E_X[X], \\
        E_{X}[X] = E_Y\left[E_{X|Y}[X|Y]\right] = E_Y\left[\frac{1}{2}\cdot \frac{1}{2}y + \frac{1}{2}\cdot 2y \right] = \frac{5}{4}E_Y[Y],
    \end{align*}
    which implies $E_X[X] = E_Y[Y] = 0$.
    \uncover<2>{
    	~\\
    	~\\
    	Barry Nalebuff, \emph{The Other Person's Envelope is Always Greener}.
    }
\end{frame}
%
% Section: Exercises
\section{Exercises}
%
% Subsection: Ex 1
\subsection{Exercise 1.}
\begin{frame}{Exercise 1.}
    \justifying
    \structb{Exercise 1.} Suppose $n$ tennis players enter a tournament. In each round of the play, if the number of players is odd, then one player is randomly selected to enter the next round. If the number of players is even, all players are randomly paired. The loser in each pair is eliminated from the tournament. This process ends until the final winner is determined. Then what is the probability that two specific players $A$ and $B$ will ever play against each other?
\end{frame}
%
% Frame: sol of the exe 1.
\begin{frame}{Exercise 1.}
    \justifying
    \structb{Solution.} The number of players eliminated will be the same as the number of matches. Therefore, exactly $n-1$ matches must be played. The number of possible pairs of players is $\dbinom{n}{2}$. Each of the two players in every match is equally likely to win. Thus before the tournament begins, every possible pair of players is equally likely to appear in each particular one of the $n-1$ matches. Accordingly, the probability that players $A$ and $B$ will meet in some particular match that is specified in advance is $1\Bigg/\dbinom{n}{2}$. Since there are $n-1$ matches where they might meet, the probability is
    \begin{align*}
        p = (n-1)\Bigg/\binom{n}{2} = \frac{2}{n}.
    \end{align*}
\end{frame}
%
% Subsection: Ex 2.
\subsection{Exercise 2.}
\begin{frame}{Exercise 2.}
    \justifying
    \structb{Exercise 2.} Suppose Keven plays the game of craps as follows. He first rolls two dice, and the sum $x$ of the two numbers is observed.
    \begin{itemize}
    	\item If $x \in \{7, 11\}$, he wins immediately.
    	\item If $x\in \{2, 3, 12\}$, he loses immediately.
    	\item Otherwise, the dice are rolled again and again until either the sum $x$ or 7 appear. He wins if $x$ first appears, and loses if $7$ first appears.
    \end{itemize}
    What is the probability of winning?
\end{frame}
%
% Frame: sol of Ex 2.
\begin{frame}[allowframebreaks]{Exercise 2.}
    \justifying
    \structb{Solution.} Denote by $W$ the event that Keven wins the game. Then the sample space $S$ contains all possible sequences of sums from the rolls of dice. Let $B_i$ the event that the sum of the first roll is $i$, where $i = 2, \ldots, 12$. Then according to the total probability formula, 
    \begin{align*}
        P(W) = \sum_{i=2}^{12} P[B_i]P[W|B_i].
    \end{align*}
    Then we need to specify $P[B_i]$ and $P[W|B_i]$. From the game rule,
    \begin{align*}
        P[W|B_2] = P[W|B_3] = P[W|B_{12}] = 0, \qquad P[W|B_7] = P[W|B_{11}] = 1.
    \end{align*}
    For the cases where $i\in \{4, 5, 6, 8, 9, 10\}$, the probability of winning is given by
    \begin{align*}
        P[W|B_i] = \frac{P[B_i]}{P[B_i\cup B_7]},
    \end{align*}
    since the game ends either by rolling a sum of $i$ or 7. We can obtain
    \begin{align*}
        & P[W|B_4] = \frac{3/36}{3/36 + 6/36} = \frac{1}{3}, \qquad P[W|B_5] = \frac{4/36}{4/36 + 6/36} = \frac{2}{5}, \\
        & P[W|B_6] = \frac{5/36}{5/36 + 6/36} = \frac{5}{11}, \qquad P[W|B_8] = \frac{5}{36}{5/36 + 6/36} = \frac{5}{11}, \\
        & P[W|B_9] = \frac{4/36}{4/36 + 6/36} = \frac{2}{5}, \qquad P[W|B_{10}] = \frac{3}{36}{3/36 + 6/36} = \frac{1}{3}.
    \end{align*}
    Therefore, the total probability is given by
    \begin{align*}
        P[W] & = \sum_{i=2}^{12} P[B_i]P[W|B_i] = 0.493.
    \end{align*}
\end{frame}
%
% Subsection: Ex 3
\subsection{Exercise 3.}
\begin{frame}{Exercise 3.}
    \justifying
    \structb{Exercise 3 (\emph{The Gambler's Ruin Problem}).} Suppose Keven and Cindy are playing a game against each other. Let $p\in (0, 1)$ denotes the probability that Keven wins each play of the game. If Keven wins one play, he wins one dollar from Cindy, and if he loses, he loses one dollar. Suppose Keven and Cindy start from $i$ and $k-i$ dollars, respectively. Keven has decided to quit the game as soon as his current fortune reaches either $k$ or 0. Then what is the probability that he finally reaches $k$ dollars?
\end{frame}
%
% Frame: sol of Exercise 3
\begin{frame}[allowframebreaks]{Exercise 3.}
    \justifying
    \structb{Solution.} Starting with $i$ and $k-i$ dollars respectively, we denote the probability that Keven will reach $k$ dollars as $a_i$. Due to independence, the setup of the game is initialized with the change only being the starting fortune. Namely, suppose after the first play, Keven owns $j$ dollars, then we get the conditional probability
    \begin{align*}
        P[W|(i, j)] = a_j,
    \end{align*}
    where $W$ denotes the event that Keven ends up with $k$ dollars, and $(i, j)$ denotes his sequence of fortune. 
    ~\\
    Particularly, if the sequence reaches 0, then the probability is 0, giving $a_0 = 0$. Similarly, $a_k = 1$. Let $A_1$ denote the event that Keven wins the first play, and let $B_1$ denote the event that he loses the first play. Then we obtain
    \begin{align*}
        P[W] & = P[A_1]P[W|A_1] + p{B_1}P[W|B_1] \\
        & = pP[W|A_1] + (1 - p)P[W|B_1],
    \end{align*}
    namely,
    \begin{align*}
        a_i = pa_{i+1} + (1-p)a_{i-1}, \qquad i = 1, \ldots, k-1
    \end{align*}
    Therefore, we obtain
    \begin{align*}
        a_1 & = pa_2, \\
        a_2 & = pa_3 + (1-p)a_1, \\
        a_3 & = pa_4 + (1-p)a_2, \\
        & \vdots \\
        a_{k-2} & = pa_{k-1} + (1-p)a_{k-3}, \\
        a_{k-1} & = p + (1-p)a_{k-2}.
    \end{align*}
    Since $a_i = pa_i + (1-p)a_i$, we have
    \begin{align*}
        a_2 - a_1 & = \frac{1-p}{p}a_1, \\
        a_3 - a_2 & = \frac{1-p}{p}(a_2 - a_1) = \left(\frac{1-p}{p} \right)^2 a_1, \\
        a_4 - a_3 & = \frac{1-p}{p}(a_3 - a_2) = \left(\frac{1-p}{p} \right)^3 a_1, \\
        & \vdots \\
        a_{k-1} - a_{k-2} & = \frac{1-p}{p}(a_{k-2} - a_{k-3}) = \left(\frac{1-p}{p} \right)^{k-2} a_1, \\
        1 - a_{k-1} & = \frac{1-p}{p}(a_{k-1} - a_{k-2}) = \left(\frac{1-p}{p} \right)^{k-1} a_1.
    \end{align*}
    Summing up all terms, we have
    \begin{align*}
        1 - a_1 = a_1 \sum_{i=1}^{k-1} \left(\frac{1-p}{p} \right)^i.
    \end{align*}
    Then based on the value of $p$, we have to discuss two cases.
    \begin{itemize}
    	\item \underline{$p = \dfrac{1}{2}$, (fair game)}. This gives
    	\begin{align*}
    	    1 - a_1 = (k-1)a_1\qquad\Rightarrow\qquad a_1 = \frac{1}{k},
    	\end{align*}
    	which then indicates
    	\begin{align*}
    	    a_i = \frac{i}{k}, \qquad i = 1, \ldots, k-1
    	\end{align*}
    	by induction.
    	\item \underline{$p\neq \frac{1}{2}$, (unfair game)}. In this case, we have
    	\begin{align*}
    	    1 - a_1 = a_1\cdot \frac{\left(\dfrac{1-p}{p} \right)^k - \left(\dfrac{1-p}{p} \right)}{\dfrac{1-p}{p} - 1} \qquad \Rightarrow \qquad a_1 = \frac{\dfrac{1-p}{p} - 1}{\left(\dfrac{1-p}{p} \right)^k - 1}.
    	\end{align*}
    	Then similarly, we have
    	\begin{align*}
    	    a_i = \frac{\left(\dfrac{1-p}{p} \right)^i - 1}{\left(\dfrac{1-p}{p}^k \right)^k - 1}, \qquad i = 1, \ldots, k-1.
    	\end{align*}
    \end{itemize}
\end{frame}

\end{document}
